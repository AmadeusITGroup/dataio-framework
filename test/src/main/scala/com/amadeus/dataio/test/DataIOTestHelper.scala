package com.amadeus.dataio.test

import com.amadeus.dataio.config.PipelineConfig
import com.amadeus.dataio.core.handler.HandlerAccessor
import com.amadeus.dataio.core.{Logging, Output}
import com.amadeus.dataio.pipes.spark.batch.SparkInput
import com.amadeus.dataio.processing.Processor
import com.typesafe.config.ConfigValueFactory
import org.apache.spark.sql.{DataFrame, SparkSession}

import scala.util.{Failure, Success, Try}

/** A trait that provides helper methods for testing data input and output operations in a Spark-based pipeline.
  * It includes functionality to create and run processors, handle their configurations, and assert output results.
  *
  * This trait is intended to be used in unit tests that need to validate the processing and output of data pipelines.
  */
trait DataIOTestHelper extends Logging {

  /** Creates a `Processor` instance from the given configuration path.
    * The configuration should contain a `processing` node with the necessary information to instantiate a processor.
    *
    * @param configPath The path to the configuration file.
    * @return The created `Processor` instance.
    * @throws Exception if no `processing` node is found or if the `Processor` instantiation fails.
    */
  def createProcessor(configPath: String): Processor = {
    val config = PipelineConfig(configPath)
    val node = config.processing.nodes.headOption.getOrElse {
      throw new Exception(s"""No `processing` node found in config "$configPath".""")
    }

    Try { Processor(node) } match {
      case Success(p) => p
      case Failure(ex) =>
        throw new Exception(s"""Failed to instantiate processor from config "$configPath".
             |Exception: ${ex.getMessage}""".stripMargin)
    }
  }

  /** Creates `Handler` instances based on the configuration from the given path.
    *
    * @param configPath The path to the configuration file.
    * @return The created `HandlerAccessor` instance.
    */
  def createHandlers(configPath: String): HandlerAccessor = {
    val config = PipelineConfig(configPath)
    HandlerAccessor(config)
  }

  /** Runs the processor described in the configuration specified in the given path.
    *
    * @param configPath The path to the configuration file.
    * @param spark      The `SparkSession` to use during processing.
    */
  def runProcessor(configPath: String)(implicit spark: SparkSession): Unit = {
    val processor = createProcessor(configPath)
    val handlers  = createHandlers(configPath)

    processor.run(handlers)
  }

  /** Runs the processor and asserts the result against the expected output.
    * It checks whether the output generated by the processor matches the expected output.
    *
    * @param configPath The path to the configuration file.
    * @param spark      The `SparkSession` to use during processing.
    * @throws Exception If any of the outputs do not have an `expected` configuration.
    * @throws AssertionError if the schemas, contents, or row counts do not match.
    */
  def assertProcessorResult(configPath: String)(implicit spark: SparkSession): Unit = {
    val processor = createProcessor(configPath)
    val handlers  = createHandlers(configPath)

    processor.run(handlers)

    handlers.output.getAll.foreach(assertOutputResult)
  }

  private def assertOutputResult(output: Output)(implicit spark: SparkSession): Unit = {
    if (!output.config.hasPath("expected"))
      throw new Exception(s""""${output.name}" output does not have an `expected` configuration.""")
    val expectedPipe = Try(output.config.getString("expected.name")) match {
      case Success(_) => SparkInput(output.config.getConfig("expected"))
      case _ =>
        val configWithName = output.config
          .getConfig("expected")
          .withValue("name", ConfigValueFactory.fromAnyRef(""))
        SparkInput(configWithName)
    }

    val expectedDf = expectedPipe.read

    val outputDf = output match {
      case testOutput: TestOutput =>
        TestDataStore.load(testOutput.path)
      case _ =>
        SparkInput(output.config).read
    }

    assertEquals(outputDf, expectedDf)
  }

  private def assertEquals(left: DataFrame, right: DataFrame): Unit = {
    val leftSchema   = left.schema
    val rightSchema  = right.schema
    val schemasMatch = leftSchema.equals(rightSchema)

    assert(
      schemasMatch,
      s"""
         |Schemas mismatch.
         |Actual: ${leftSchema.treeString}
         |Expected: ${rightSchema.treeString}
         |""".stripMargin
    )

    val leftExceptRight  = left.except(right)
    val rightExceptLeft  = right.except(left)
    val contentsAreEqual = leftExceptRight.isEmpty && rightExceptLeft.isEmpty

    assert(
      contentsAreEqual,
      s"""
         |Content mismatch.
         |Unexpected rows: ${if (leftExceptRight.isEmpty) "(empty)"
      else leftExceptRight.toJSON.take(5).map(s => s"\n\t $s").mkString("")}
         |Missing rows: ${if (rightExceptLeft.isEmpty) "(empty))"
      else rightExceptLeft.toJSON.take(5).map(s => s"\n\t $s").mkString("")}
         |""".stripMargin
    )

    val countMatch = left.count() == right.count()
    assert(
      countMatch,
      s"""
         |Row counts mismatch.
         |Actual: ${left.count()}
         |Expected: ${right.count()}
         |""".stripMargin
    )
  }
}
